{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matakahas/portfolio/blob/main/reddit_proed_pt1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3d4dc91",
      "metadata": {
        "papermill": {
          "duration": 0.021671,
          "end_time": "2022-01-03T23:12:45.696903",
          "exception": false,
          "start_time": "2022-01-03T23:12:45.675232",
          "status": "completed"
        },
        "tags": [],
        "id": "e3d4dc91"
      },
      "source": [
        "## Topic modeling and flair prediction from the banned r/proED/ subreddit (Part 1)\n",
        "In Part 1 of this project, I will scrape the [mirror site](https://goutiest-zorse-5012.dataplicity.io/) of r/proED and save the obtained dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6d94bea",
      "metadata": {
        "papermill": {
          "duration": 0.020081,
          "end_time": "2022-01-03T23:12:45.738885",
          "exception": false,
          "start_time": "2022-01-03T23:12:45.718804",
          "status": "completed"
        },
        "tags": [],
        "id": "a6d94bea"
      },
      "source": [
        "### required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1a3b715",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-03T23:12:45.784181Z",
          "iopub.status.busy": "2022-01-03T23:12:45.782948Z",
          "iopub.status.idle": "2022-01-03T23:12:46.096125Z",
          "shell.execute_reply": "2022-01-03T23:12:46.095327Z",
          "shell.execute_reply.started": "2022-01-03T23:03:55.525185Z"
        },
        "papermill": {
          "duration": 0.336969,
          "end_time": "2022-01-03T23:12:46.096321",
          "exception": false,
          "start_time": "2022-01-03T23:12:45.759352",
          "status": "completed"
        },
        "tags": [],
        "id": "d1a3b715"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests \n",
        "import html5lib\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d041b23",
      "metadata": {
        "papermill": {
          "duration": 0.020014,
          "end_time": "2022-01-03T23:12:46.136789",
          "exception": false,
          "start_time": "2022-01-03T23:12:46.116775",
          "status": "completed"
        },
        "tags": [],
        "id": "8d041b23"
      },
      "source": [
        "### scraping\n",
        "I will scrape posts from an entire time period (2015-2018). Here is the function for conducting the scraping and parsing the data into a cleaner format (I will do a bit more pre-processing in Part 2)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def proed_scraping(link, headers):\n",
        "    url = \"https://goutiest-zorse-5012.dataplicity.io/\" + link\n",
        "    search_response=requests.get(url, headers = headers)\n",
        "    if search_response.status_code == 200:\n",
        "        soup=BeautifulSoup(search_response.content,'html.parser')\n",
        "    else:\n",
        "        raise Exception(f\"request is not processed correctly. Code: {search_response.status_code}\")\n",
        "    #extract titles\n",
        "    titles = []\n",
        "    for tag in soup.find_all('strong'):\n",
        "        titles.append(tag.text)\n",
        "    #extract flairs\n",
        "    flairs = []\n",
        "    new_titles = []\n",
        "    for t in titles:\n",
        "        pattern = re.match(r\"\\[[a-z\\/?A-Z]+\\]\", t)\n",
        "        if pattern:\n",
        "            flairs.append(re.sub(r\"\\[|\\]\", '', pattern.group(0)))\n",
        "            new_titles.append(re.sub(r\"\\[[a-z\\/?A-Z]+\\]\\s\", '', t))\n",
        "        else:\n",
        "            flairs.append('none')\n",
        "            new_titles.append(t)\n",
        "    #make a dataframe\n",
        "    df = pd.DataFrame(data={'Flair':flairs, 'Title':new_titles})\n",
        "    #extract usenames, dates, and texts\n",
        "    users = []\n",
        "    dates = []\n",
        "    texts = []\n",
        "    txt = []\n",
        "    count = 0\n",
        "    for br in soup.find_all(\"br\"):\n",
        "        next_s = br.nextSibling\n",
        "        if str(next_s) == '<hr/>':\n",
        "            texts.append(txt)\n",
        "            txt = []\n",
        "            count = 0\n",
        "        else:\n",
        "            count += 1\n",
        "            if count == 1:\n",
        "                users.append(next_s)\n",
        "            elif count == 2:\n",
        "                dates.append(next_s)\n",
        "            elif count >= 5:\n",
        "                txt.append(next_s)\n",
        "    #remove the line break tags from each text \n",
        "    new_texts = []\n",
        "    for text in texts:\n",
        "        new_text = ''.join(str(t) for t in text if t.name != 'br')\n",
        "        new_texts.append(new_text)\n",
        "    #append the extracted data to the dataframe\n",
        "    df['User'], df['Date'], df['Text'] = [pd.Series(users), pd.Series(dates), pd.Series(new_texts)]\n",
        "    #parse the info under the Date column\n",
        "    df['Date'] = df['Date'].map(lambda x: format_dt(str(x)))\n",
        "    #finally, export the dataframe\n",
        "    l = link.split('/')[1]\n",
        "    df.to_csv(f'{l}.csv')"
      ],
      "metadata": {
        "id": "ZBVHS-pPuL00"
      },
      "id": "ZBVHS-pPuL00",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, get a list of log URLs and scrape posts from each of them."
      ],
      "metadata": {
        "id": "JGjJbZ8276Y8"
      },
      "id": "JGjJbZ8276Y8"
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://goutiest-zorse-5012.dataplicity.io/\"\n",
        "\n",
        "headers = {'User-Agent': 'Mozilla 5.0'}\n",
        "\n",
        "search_response=requests.get(url, headers = headers)\n",
        "\n",
        "if search_response.status_code == 200:\n",
        "    soup=BeautifulSoup(search_response.content,'html.parser')"
      ],
      "metadata": {
        "id": "oRLJSJCZ8EPa"
      },
      "id": "oRLJSJCZ8EPa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b839b03",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-03T23:12:51.511259Z",
          "iopub.status.busy": "2022-01-03T23:12:51.510397Z",
          "iopub.status.idle": "2022-01-03T23:12:51.516357Z",
          "shell.execute_reply": "2022-01-03T23:12:51.517001Z",
          "shell.execute_reply.started": "2022-01-03T23:04:16.645815Z"
        },
        "papermill": {
          "duration": 0.03497,
          "end_time": "2022-01-03T23:12:51.517191",
          "exception": false,
          "start_time": "2022-01-03T23:12:51.482221",
          "status": "completed"
        },
        "tags": [],
        "id": "0b839b03",
        "outputId": "f09fe080-c339-48d6-e5e7-9f7015f8b48e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ProED_Summary/15_05_16_22_31-15_09_05_07_48.html',\n",
              " 'ProED_Summary/15_09_05_08_00-16_04_04_10_03.html',\n",
              " 'ProED_Summary/16_04_04_10_30-16_08_06_19_39.html',\n",
              " 'ProED_Summary/16_08_06_19_49-16_11_11_05_11.html',\n",
              " 'ProED_Summary/16_11_11_05_11-17_03_14_00_34.html',\n",
              " 'ProED_Summary/17_03_14_00_54-17_06_17_00_26.html',\n",
              " 'ProED_Summary/17_06_17_00_29-17_08_23_12_34.html',\n",
              " 'ProED_Summary/17_08_23_12_42-17_11_05_19_32.html',\n",
              " 'ProED_Summary/17_11_05_19_38-18_01_21_11_15.html',\n",
              " 'ProED_Summary/18_01_21_11_21-18_04_02_11_12.html',\n",
              " 'ProED_Summary/18_04_02_11_45-18_06_04_09_55.html',\n",
              " 'ProED_Summary/18_06_04_10_06-18_07_24_13_56.html',\n",
              " 'ProED_Summary/18_07_24_14_09-18_09_06_20_56.html',\n",
              " 'ProED_Summary/18_09_06_21_30-18_10_14_09_13.html',\n",
              " 'ProED_Summary/18_10_14_09_15-18_11_14_15_33.html']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "links = []\n",
        "for link in soup.findAll('a'):\n",
        "    links.append(link.get('href'))\n",
        "links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21968b79",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-03T23:12:51.571857Z",
          "iopub.status.busy": "2022-01-03T23:12:51.571217Z",
          "iopub.status.idle": "2022-01-03T23:14:10.734049Z",
          "shell.execute_reply": "2022-01-03T23:14:10.733456Z",
          "shell.execute_reply.started": "2022-01-03T23:06:25.959572Z"
        },
        "papermill": {
          "duration": 79.191283,
          "end_time": "2022-01-03T23:14:10.734237",
          "exception": false,
          "start_time": "2022-01-03T23:12:51.542954",
          "status": "completed"
        },
        "tags": [],
        "id": "21968b79",
        "outputId": "f3800cd9-b24e-42f2-d806-80fa916c5a84"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:125: FutureWarning: Inferring datetime64[ns] from data containing strings is deprecated and will be removed in a future version. To retain the old behavior explicitly pass Series(data, dtype={value.dtype})\n"
          ]
        }
      ],
      "source": [
        "#don't forget to specify the user agent\n",
        "headers = {'User-Agent': 'Mozilla 5.0'}\n",
        "\n",
        "for link in links:\n",
        "    proana_scraping(link, headers)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I then combined all the generated datasets into one, and saved it as \"proED_full_dataset.csv,\" which I will call in Part 2. \\\n",
        "\n",
        "**EDIT:** The mirror site seems to be down, so the above code won't work. I'm not sure if it is temporary or not, but meanwhile you can download the zip file of the dataset I made from my github repository, or from [Kaggle](https://www.kaggle.com/matakahas/reddit-proana-dataset). \\\n",
        "\\\n",
        "That's it for Part 1 - thanks for tagging along!"
      ],
      "metadata": {
        "id": "LmpMVwGD82Al"
      },
      "id": "LmpMVwGD82Al"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zRpTXJPa9XrX"
      },
      "id": "zRpTXJPa9XrX",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 96.111025,
      "end_time": "2022-01-03T23:14:11.826502",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-01-03T23:12:35.715477",
      "version": "2.3.3"
    },
    "colab": {
      "name": "reddit-proed_pt1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}